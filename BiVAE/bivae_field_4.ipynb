{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilateral Variational Autoencoder (BiVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DanielShen/opt/anaconda3/envs/microsoft_rec/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM model is only supported on Linux.\n",
      "Windows executable can be found at http://www.libfm.org.\n",
      "System version: 3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:51:59) \n",
      "[Clang 14.0.6 ]\n",
      "PyTorch version: 1.13.1\n",
      "Cornac version: 1.15.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import cornac\n",
    "import pandas as pd\n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from recommenders.models.cornac.cornac_utils import predict_ranking\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "print(\"Cornac version: {}\".format(cornac.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_FILE_NAME = \"../Data/20230721T041206_sales_2023_basic_single_events_removed.csv\"\n",
    "THIS_ENGINE_NAME = \"bivae_field_4\"\n",
    "\n",
    "# country\n",
    "COUNTRY = \"nigeria\"\n",
    "\n",
    "# top k items to recommend, for train & test\n",
    "TOP_K_SPLIT_TRAIN_TEST = 10\n",
    "# top k items to recommend, for final product recommendation output\n",
    "TOP_K_WHOLE = 100\n",
    "SAVE_RECS = False\n",
    "EXTRA_COLS = False\n",
    "\n",
    "# fraction of location_skus to include in training dataset\n",
    "TRAIN_FRAC = 0.75\n",
    "\n",
    "# Model parameters\n",
    "LATENT_DIM = 50\n",
    "ENCODER_DIMS = [100]\n",
    "ACT_FUNC = \"tanh\"\n",
    "LIKELIHOOD = \"pois\"\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# top MULTIPLIER * k items are considered relevant for nDCG\n",
    "RNDCG_MULTIPLIER = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revised/copied code from packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from recommenders.utils.constants import (\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ITEM_COL,\n",
    "    DEFAULT_RATING_COL,\n",
    "    DEFAULT_PREDICTION_COL,\n",
    "    DEFAULT_RELEVANCE_COL,\n",
    "    DEFAULT_SIMILARITY_COL,\n",
    "    DEFAULT_ITEM_FEATURES_COL,\n",
    "    DEFAULT_ITEM_SIM_MEASURE,\n",
    "    DEFAULT_K,\n",
    "    DEFAULT_THRESHOLD,\n",
    ")\n",
    "DEFAULT_RNDCG_MULTIPLIER = 3\n",
    "\n",
    "\n",
    "def _get_rating_column(relevancy_method: str, **kwargs) -> str:\n",
    "    r\"\"\"Helper utility to simplify the arguments of eval metrics\n",
    "    Attemtps to address https://github.com/microsoft/recommenders/issues/1737.\n",
    "\n",
    "    Args:\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "\n",
    "    Returns:\n",
    "        str: rating column name.\n",
    "    \"\"\"\n",
    "    if relevancy_method != \"top_k\":\n",
    "        if \"col_rating\" not in kwargs:\n",
    "            raise ValueError(\"Expected an argument `col_rating` but wasn't found.\")\n",
    "        col_rating = kwargs.get(\"col_rating\")\n",
    "    else:\n",
    "        col_rating = kwargs.get(\"col_rating\", DEFAULT_RATING_COL)\n",
    "    return col_rating\n",
    "\n",
    "\n",
    "def get_top_k_items(\n",
    "    dataframe, col_user=DEFAULT_USER_COL, col_rating=DEFAULT_RATING_COL, k=DEFAULT_K\n",
    "):\n",
    "    \"\"\"Get the input customer-item-rating tuple in the format of Pandas\n",
    "    DataFrame, output a Pandas DataFrame in the dense format of top k items\n",
    "    for each user.\n",
    "\n",
    "    Note:\n",
    "        If it is implicit rating, just append a column of constants to be\n",
    "        ratings.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): DataFrame of rating data (in the format\n",
    "        customerID-itemID-rating)\n",
    "        col_user (str): column name for user\n",
    "        col_rating (str): column name for rating\n",
    "        k (int or None): number of items for each user; None means that the input has already been\n",
    "        filtered out top k items and sorted by ratings and there is no need to do that again.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame of top k items for each user, sorted by `col_user` and `rank`\n",
    "    \"\"\"\n",
    "    # Sort dataframe by col_user and (top k) col_rating\n",
    "    if k is None:\n",
    "        top_k_items = dataframe\n",
    "    else:\n",
    "        top_k_items = (\n",
    "            dataframe.sort_values([col_user, col_rating], ascending=[True, False])\n",
    "            .groupby(col_user, as_index=False)\n",
    "            .head(k)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    # Add ranks\n",
    "    top_k_items[\"rank\"] = top_k_items.groupby(col_user, sort=False).cumcount() + 1\n",
    "    return top_k_items\n",
    "\n",
    "\n",
    "def merge_ranking_true_pred_new(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user,\n",
    "    col_item,\n",
    "    col_rating,\n",
    "    col_prediction,\n",
    "    relevancy_method,\n",
    "    k=DEFAULT_K,\n",
    "    rndcg_multiplier=DEFAULT_RNDCG_MULTIPLIER,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "):\n",
    "    \"\"\"Filter truth and prediction data frames on common users\n",
    "\n",
    "    Args:\n",
    "        rating_true (pandas.DataFrame): True DataFrame\n",
    "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
    "        col_user (str): column name for user\n",
    "        col_item (str): column name for item\n",
    "        col_rating (str): column name for rating\n",
    "        col_prediction (str): column name for prediction\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "        k (int): number of top k items per user (optional)\n",
    "        threshold (float): threshold of top items per user (optional)\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame of recommendation hits, sorted by `col_user` and `rank`\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the prediction and true data frames have the same set of users\n",
    "    common_users = set(rating_true[col_user]).intersection(set(rating_pred[col_user]))\n",
    "    rating_true_common = rating_true[rating_true[col_user].isin(common_users)]\n",
    "    rating_pred_common = rating_pred[rating_pred[col_user].isin(common_users)]\n",
    "\n",
    "    # Return hit items in prediction data frame with ranking information. This is used for calculating NDCG and MAP.\n",
    "    # Use first to generate unique ranking values for each item. This is to align with the implementation in\n",
    "    # Spark evaluation metrics, where index of each recommended items (the indices are unique to items) is used\n",
    "    # to calculate penalized precision of the ordered items.\n",
    "    if relevancy_method == \"top_k\":\n",
    "        top_k = k\n",
    "    elif relevancy_method == \"by_threshold\":\n",
    "        top_k = threshold\n",
    "    elif relevancy_method is None:\n",
    "        top_k = None\n",
    "    else:\n",
    "        raise NotImplementedError(\"Invalid relevancy_method\")\n",
    "    df_hit = get_top_k_items(\n",
    "        dataframe=rating_pred_common,\n",
    "        col_user=col_user,\n",
    "        col_rating=col_prediction,\n",
    "        k=top_k,\n",
    "    )\n",
    "    rating_true_common_top_mult_k = get_top_k_items(\n",
    "        dataframe=rating_true_common,\n",
    "        col_user=col_user,\n",
    "        col_rating=col_rating,\n",
    "        k=RNDCG_MULTIPLIER * top_k,\n",
    "    )[[col_user, col_item]]\n",
    "    df_hit = pd.merge(df_hit, rating_true_common_top_mult_k, on=[col_user, col_item])[\n",
    "        [col_user, col_item, \"rank\"]\n",
    "    ]\n",
    "\n",
    "    return df_hit\n",
    "\n",
    "\n",
    "def rndcg_at_k(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=DEFAULT_K,\n",
    "    rndcg_multiplier=RNDCG_MULTIPLIER,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "    score_type=\"binary\",\n",
    "    discfun_type=\"loge\",\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain (nDCG).\n",
    "\n",
    "    Info: https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
    "\n",
    "    Args:\n",
    "        rating_true (pandas.DataFrame): True DataFrame\n",
    "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
    "        col_user (str): column name for user\n",
    "        col_item (str): column name for item\n",
    "        col_rating (str): column name for rating\n",
    "        col_prediction (str): column name for prediction\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "        k (int): number of top k items per user\n",
    "        threshold (float): threshold of top items per user (optional)\n",
    "        score_type (str): type of relevance scores ['binary', 'raw', 'exp']. With the default option 'binary', the\n",
    "            relevance score is reduced to either 1 (hit) or 0 (miss). Option 'raw' uses the raw relevance score.\n",
    "            Option 'exp' uses (2 ** RAW_RELEVANCE - 1) as the relevance score\n",
    "        discfun_type (str): type of discount function ['loge', 'log2'] used to calculate DCG.\n",
    "\n",
    "    Returns:\n",
    "        float: nDCG at k (min=0, max=1).\n",
    "    \"\"\"\n",
    "    col_rating = _get_rating_column(relevancy_method, **kwargs)\n",
    "    df_hit = merge_ranking_true_pred_new(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "        relevancy_method=relevancy_method,\n",
    "        k=k,\n",
    "        rndcg_multiplier=rndcg_multiplier,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    df_dcg = df_hit.merge(rating_pred, on=[col_user, col_item]).merge(\n",
    "        rating_true, on=[col_user, col_item], how=\"outer\", suffixes=(\"_left\", None)\n",
    "    )\n",
    "\n",
    "    if score_type == \"binary\":\n",
    "        df_dcg[\"rel\"] = 1\n",
    "    elif score_type == \"raw\":\n",
    "        df_dcg[\"rel\"] = df_dcg[col_rating]\n",
    "    elif score_type == \"exp\":\n",
    "        df_dcg[\"rel\"] = 2 ** df_dcg[col_rating] - 1\n",
    "    else:\n",
    "        raise ValueError(\"score_type must be one of 'binary', 'raw', 'exp'\")\n",
    "\n",
    "    if discfun_type == \"loge\":\n",
    "        discfun = np.log\n",
    "    elif discfun_type == \"log2\":\n",
    "        discfun = np.log2\n",
    "    else:\n",
    "        raise ValueError(\"discfun_type must be one of 'loge', 'log2'\")\n",
    "\n",
    "    # Calculate the actual discounted gain for each record\n",
    "    df_dcg[\"dcg\"] = df_dcg[\"rel\"] / discfun(1 + df_dcg[\"rank\"])\n",
    "\n",
    "    # Calculate the ideal discounted gain for each record\n",
    "    df_idcg = df_dcg.sort_values([col_user, col_rating], ascending=False)\n",
    "    df_idcg[\"irank\"] = df_idcg.groupby(col_user, as_index=False, sort=False)[\n",
    "        col_rating\n",
    "    ].rank(\"first\", ascending=False)\n",
    "    df_idcg[\"idcg\"] = df_idcg[\"rel\"] / discfun(1 + df_idcg[\"irank\"])\n",
    "\n",
    "    # Calculate the actual DCG for each user\n",
    "    df_user = df_dcg.groupby(col_user, as_index=False, sort=False).agg({\"dcg\": \"sum\"})\n",
    "\n",
    "    # Calculate the ideal DCG for each user\n",
    "    df_user = df_user.merge(\n",
    "        df_idcg.groupby(col_user, as_index=False, sort=False)\n",
    "        .head(k)\n",
    "        .groupby(col_user, as_index=False, sort=False)\n",
    "        .agg({\"idcg\": \"sum\"}),\n",
    "        on=col_user,\n",
    "    )\n",
    "\n",
    "    # DCG over IDCG is the normalized DCG\n",
    "    df_user[\"ndcg\"] = df_user[\"dcg\"] / df_user[\"idcg\"]\n",
    "    return df_user[\"ndcg\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data\n",
    "\n",
    "\n",
    "### 1.1 Load and split data\n",
    "\n",
    "To evaluate the performance of item recommendation, we adopted the provided `python_random_split` tool for the consistency.  Data is randomly split into training and test sets with the ratio of 75/25.\n",
    "\n",
    "\n",
    "Note that Cornac also cover different [built-in schemes](https://cornac.readthedocs.io/en/latest/eval_methods.html) for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>product</th>\n",
       "      <th>sl_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>\"01aa6102-a054-4f25-a747-a39a4ea86769\"</td>\n",
       "      <td>Mixanal Tablet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>\"01aa6102-a054-4f25-a747-a39a4ea86769\"</td>\n",
       "      <td>Emzor Paracetamol 500mg Tablets x96</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>\"01aa6102-a054-4f25-a747-a39a4ea86769\"</td>\n",
       "      <td>Syrup Paracetamol 125mg/5ml (Emzor)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>\"01aa6102-a054-4f25-a747-a39a4ea86769\"</td>\n",
       "      <td>Em-Vit-C 100ml Syrup</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>\"01aa6102-a054-4f25-a747-a39a4ea86769\"</td>\n",
       "      <td>Funbact A 30g Cream</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                location_id  \\\n",
       "509  \"01aa6102-a054-4f25-a747-a39a4ea86769\"   \n",
       "510  \"01aa6102-a054-4f25-a747-a39a4ea86769\"   \n",
       "511  \"01aa6102-a054-4f25-a747-a39a4ea86769\"   \n",
       "512  \"01aa6102-a054-4f25-a747-a39a4ea86769\"   \n",
       "513  \"01aa6102-a054-4f25-a747-a39a4ea86769\"   \n",
       "\n",
       "                                 product  sl_sold  \n",
       "509                       Mixanal Tablet        1  \n",
       "510  Emzor Paracetamol 500mg Tablets x96        3  \n",
       "511  Syrup Paracetamol 125mg/5ml (Emzor)       20  \n",
       "512                 Em-Vit-C 100ml Syrup        2  \n",
       "513                  Funbact A 30g Cream        2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all_cols = pd.read_csv(DATA_FILE_NAME)\n",
    "data_all_cols = data_all_cols[data_all_cols[\"country\"] == COUNTRY]\n",
    "\n",
    "data = data_all_cols[[\"location_id\", \"product\", \"sl_sold\"]]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use quantity sold, not revenue of sales, for each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_random_split(data, TRAIN_FRAC, seed = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cornac Dataset\n",
    "\n",
    "To work with models implemented in Cornac, we need to construct an object from [Dataset](https://cornac.readthedocs.io/en/latest/data.html#module-cornac.data.dataset) class.\n",
    "\n",
    "Dataset Class in Cornac serves as the main object that the models will interact with.  In addition to data transformations, Dataset provides a bunch of useful iterators for looping through the data, as well as supporting different negative sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 406\n",
      "Number of items: 1545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DanielShen/opt/anaconda3/envs/microsoft_rec/lib/python3.8/site-packages/cornac/data/dataset.py:361: UserWarning: 32 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    }
   ],
   "source": [
    "train_set = cornac.data.Dataset.from_uir(train.itertuples(index=False), seed=SEED)\n",
    "\n",
    "print('Number of users: {}'.format(train_set.num_users))\n",
    "print('Number of items: {}'.format(train_set.num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training\n",
    "\n",
    "The BiVAE has a few important parameters that we need to consider:\n",
    "\n",
    "- `k`: dimension of the latent space (i.e. the size of $\\bf{\\theta}_u$  and  $\\bf{\\beta}_i$ ).\n",
    "- `encoder_structure`: dimension(s) of hidden layer(s) of the user and item encoders.\n",
    "- `act_fn`: non-linear activation function used in the encoders.\n",
    "- `likelihood`: choice of the likelihood function being optimized.\n",
    "- `n_epochs`: number of passes through training data.\n",
    "- `batch_size`: size of mini-batches of data during training.\n",
    "- `learning_rate`: step size in the gradient update rules.\n",
    "\n",
    "To train the model, we simply need to call the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivae = cornac.models.BiVAECF(\n",
    "    k=LATENT_DIM,\n",
    "    encoder_structure=ENCODER_DIMS,\n",
    "    act_fn=ACT_FUNC,\n",
    "    likelihood=LIKELIHOOD,\n",
    "    n_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    seed=SEED,\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:52<00:00,  2.89it/s, loss_i=0.544, loss_u=2.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 173.0806 seconds for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    bivae.fit(train_set)\n",
    "print(\"Took {} seconds for training.\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Prediction\n",
    "\n",
    "Now that our model is trained, we can produce the ranked lists for recommendation.  Every recommender models in Cornac provide `rate()` and `rank()` methods for predicting item rated value as well as item ranked list for a given user.  To make use of the current evaluation schemes, we will through `predict()` and `predict_ranking()` functions inside `cornac_utils` to produce the predictions.\n",
    "\n",
    "Note that BiVAE model is effectively designed for item ranking.  Hence, we only measure the performance using ranking metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.3780 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    all_predictions = predict_ranking(bivae, train, usercol='location_id', itemcol='product', remove_seen=True)\n",
    "print(\"Took {} seconds for prediction.\".format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>product</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34575</th>\n",
       "      <td>\"72d85136-5b62-48d2-a677-c5b10e091f2a\"</td>\n",
       "      <td>Ibuprofen (Afrab) Syrup</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34576</th>\n",
       "      <td>\"72d85136-5b62-48d2-a677-c5b10e091f2a\"</td>\n",
       "      <td>Levofem Tablets</td>\n",
       "      <td>0.372398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34577</th>\n",
       "      <td>\"72d85136-5b62-48d2-a677-c5b10e091f2a\"</td>\n",
       "      <td>Zinnat Tablets 500mg x10</td>\n",
       "      <td>0.000160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34578</th>\n",
       "      <td>\"72d85136-5b62-48d2-a677-c5b10e091f2a\"</td>\n",
       "      <td>Swibetic 5/500mg x30 Tablet</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34579</th>\n",
       "      <td>\"72d85136-5b62-48d2-a677-c5b10e091f2a\"</td>\n",
       "      <td>Emzolyn Cough Syrup Adult</td>\n",
       "      <td>0.863337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  location_id                      product  \\\n",
       "34575  \"72d85136-5b62-48d2-a677-c5b10e091f2a\"      Ibuprofen (Afrab) Syrup   \n",
       "34576  \"72d85136-5b62-48d2-a677-c5b10e091f2a\"              Levofem Tablets   \n",
       "34577  \"72d85136-5b62-48d2-a677-c5b10e091f2a\"     Zinnat Tablets 500mg x10   \n",
       "34578  \"72d85136-5b62-48d2-a677-c5b10e091f2a\"  Swibetic 5/500mg x30 Tablet   \n",
       "34579  \"72d85136-5b62-48d2-a677-c5b10e091f2a\"    Emzolyn Cough Syrup Adult   \n",
       "\n",
       "       prediction  \n",
       "34575    0.000094  \n",
       "34576    0.372398  \n",
       "34577    0.000160  \n",
       "34578    0.000068  \n",
       "34579    0.863337  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Evaluation / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.061778\n",
      "NDCG:\t0.286860\n",
      "Precision@K:\t0.313555\n",
      "Recall@K:\t0.109058\n"
     ]
    }
   ],
   "source": [
    "eval_map = map_at_k(test, all_predictions, col_user='location_id', col_item='product', col_rating='sl_sold', col_prediction='prediction', k=TOP_K_SPLIT_TRAIN_TEST)\n",
    "eval_ndcg = ndcg_at_k(test, all_predictions, col_user='location_id', col_item='product', col_rating='sl_sold', col_prediction='prediction', k=TOP_K_SPLIT_TRAIN_TEST)\n",
    "eval_rndcg = rndcg_at_k(test, all_predictions, col_user='location_id', col_item='product', col_rating='sl_sold', col_prediction='prediction', k=TOP_K_SPLIT_TRAIN_TEST, rndcg_multiplier=RNDCG_MULTIPLIER)\n",
    "eval_precision = precision_at_k(test, all_predictions, col_user='location_id', col_item='product', col_rating='sl_sold', col_prediction='prediction', k=TOP_K_SPLIT_TRAIN_TEST)\n",
    "eval_recall = recall_at_k(test, all_predictions, col_user='location_id', col_item='product', col_rating='sl_sold', col_prediction='prediction', k=TOP_K_SPLIT_TRAIN_TEST)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"RNDCG:\\t%f\" % eval_rndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Train, Predict, and Evaluate on Whole Dataset\n",
    "\n",
    "Earlier, we had split train (for model training) and test (for evaluation). In implementation, we have train = whole dataset, and we can evaluate on test = whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DanielShen/opt/anaconda3/envs/microsoft_rec/lib/python3.8/site-packages/cornac/data/dataset.py:361: UserWarning: 45 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 406\n",
      "Number of items: 1583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:15<00:00,  3.70it/s, loss_i=0.608, loss_u=2.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 135.2190 seconds for training.\n",
      "Took 0.1391 seconds for prediction.\n",
      "\n",
      "MAP:\t0.323119\n",
      "NDCG:\t0.641609\n",
      "Precision@K:\t0.505320\n",
      "Recall@K:\t0.546611\n"
     ]
    }
   ],
   "source": [
    "## Data\n",
    "data_set = cornac.data.Dataset.from_uir(data.itertuples(index=False), seed=SEED)\n",
    "print('Number of users: {}'.format(data_set.num_users))\n",
    "print('Number of items: {}'.format(data_set.num_items))\n",
    "\n",
    "## Train\n",
    "bivae_whole = cornac.models.BiVAECF(\n",
    "    k=LATENT_DIM,\n",
    "    encoder_structure=ENCODER_DIMS,\n",
    "    act_fn=ACT_FUNC,\n",
    "    likelihood=LIKELIHOOD,\n",
    "    n_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    seed=SEED,\n",
    "    use_gpu=torch.cuda.is_available(),\n",
    "    verbose=True\n",
    ")\n",
    "with Timer() as t:\n",
    "    bivae_whole.fit(data_set)\n",
    "print(\"Took {} seconds for training.\".format(t))\n",
    "\n",
    "## Predict\n",
    "with Timer() as t:\n",
    "    all_predictions_whole = predict_ranking(bivae_whole, data, usercol='location_id', itemcol='product', remove_seen=False)\n",
    "print(\"Took {} seconds for prediction.\".format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.107153\n",
      "NDCG:\t0.383332\n",
      "Precision@K:\t0.618350\n",
      "Recall@K:\t0.164205\n"
     ]
    }
   ],
   "source": [
    "#TOP_K_WHOLE = 20\n",
    "#EXTRA_COLS = True\n",
    "#RNDCG_MULTIPLIER = 2\n",
    "\n",
    "## Evaluate\n",
    "eval_map_whole = map_at_k(data, all_predictions_whole, col_user='location_id', col_item='product', col_rating='sl_sold', col_prediction='prediction', k=TOP_K_WHOLE)\n",
    "eval_ndcg_whole = ndcg_at_k(data, all_predictions_whole, col_user='location_id', col_item='product', col_rating='sl_sold', col_prediction='prediction', k=TOP_K_WHOLE)\n",
    "eval_rndcg_whole = rndcg_at_k(data, all_predictions_whole, col_user='location_id', col_item='product', col_rating='sl_sold', col_prediction='prediction', k=TOP_K_WHOLE, rndcg_multiplier=RNDCG_MULTIPLIER)\n",
    "eval_precision_whole = precision_at_k(data, all_predictions_whole, col_user='location_id', col_item='product', col_rating='sl_sold', col_prediction='prediction', k=TOP_K_WHOLE)\n",
    "eval_recall_whole = recall_at_k(data, all_predictions_whole, col_user='location_id', col_item='product', col_rating='sl_sold', col_prediction='prediction', k=TOP_K_WHOLE)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map_whole,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg_whole,\n",
    "      \"Precision@K:\\t%f\" % eval_precision_whole,\n",
    "      \"Recall@K:\\t%f\" % eval_recall_whole, sep='\\n')\n",
    "\n",
    "## Save recommendations\n",
    "if SAVE_RECS:\n",
    "    top_k_predictions_whole = get_top_k_items(all_predictions_whole, col_user='location_id', col_rating='prediction', k=TOP_K_WHOLE)\n",
    "    top_k_predictions_whole.drop(\"prediction\", axis = 1, inplace = True)\n",
    "    # add column of true sales and rank of predicted products\n",
    "    top_all_true = get_top_k_items(data, col_user='location_id', col_rating='sl_sold', k=data_set.num_items)\n",
    "    top_k_predictions_whole = top_k_predictions_whole.merge(top_all_true, how = 'left', on = ['location_id', 'product'], suffixes = (None, \"_true\"))\n",
    "    top_k_predictions_whole[\"rank_true\"] = top_k_predictions_whole[\"rank_true\"].convert_dtypes()\n",
    "    top_k_predictions_whole.rename(columns = {\"product\": \"predicted product\", \"sl_sold\": \"predicted product's true sales\", \"rank_true\": \"predicted product's true rank\"}, inplace = True)\n",
    "    # add columns of true top-ranked products\n",
    "    top_all_true.rename(columns = {\"product\": \"true product\", \"sl_sold\": \"true product's sales\"}, inplace = True)\n",
    "    top_k_predictions_whole = top_k_predictions_whole.merge(top_all_true, how = 'left', on = ['location_id', 'rank'])\n",
    "    # reorder columns\n",
    "    top_k_predictions_whole = top_k_predictions_whole[[\"location_id\", \"rank\", \"true product\", \"true product's sales\", \"predicted product\", \"predicted product's true sales\", \"predicted product's true rank\"]]\n",
    "    if EXTRA_COLS:\n",
    "        # add column of fraction of locations selling predicted product\n",
    "        top_k_predictions_whole[\"predicted product sells at fraction of locations\"] = top_k_predictions_whole.apply(lambda row: round(n_locs_w_prod[row[\"predicted product\"]] / n_locs, 2), axis = 1)\n",
    "        # add column of average rank of predicted product at locations selling it\n",
    "        prod_avg_rank = {prod: round(top_all_true[top_all_true[\"true product\"] == prod][\"rank\"].mean()) for prod in n_locs_w_prod}\n",
    "        top_k_predictions_whole[\"predicted product's average rank at locations selling it\"] = top_k_predictions_whole.apply(lambda row: prod_avg_rank[row[\"predicted product\"]], axis = 1)\n",
    "        # save to csv\n",
    "        top_k_predictions_whole.to_csv(THIS_ENGINE_NAME + \"_\" + COUNTRY + \"_top_\" + str(TOP_K_WHOLE) + \"_prod_recs_extra_cols.csv\")\n",
    "    else:\n",
    "        top_k_predictions_whole.to_csv(THIS_ENGINE_NAME + \"_\" + COUNTRY + \"_top_\" + str(TOP_K_WHOLE) + \"_prod_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
