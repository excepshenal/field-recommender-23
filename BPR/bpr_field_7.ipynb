{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Personalized Ranking (BPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DanielShen/opt/anaconda3/envs/microsoft_rec/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM model is only supported on Linux.\n",
      "Windows executable can be found at http://www.libfm.org.\n",
      "System version: 3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:51:59) \n",
      "[Clang 14.0.6 ]\n",
      "Cornac version: 1.15.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import cornac\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from recommenders.models.cornac.cornac_utils import predict_ranking\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.utils.constants import (\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ITEM_COL,\n",
    "    DEFAULT_RATING_COL,\n",
    "    DEFAULT_PREDICTION_COL,\n",
    "    DEFAULT_RELEVANCE_COL,\n",
    "    DEFAULT_SIMILARITY_COL,\n",
    "    DEFAULT_ITEM_FEATURES_COL,\n",
    "    DEFAULT_ITEM_SIM_MEASURE,\n",
    "    DEFAULT_K,\n",
    "    DEFAULT_THRESHOLD,\n",
    ")\n",
    "DEFAULT_RNDCG_MULTIPLIER = 3\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Cornac version: {}\".format(cornac.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "THIS_ENGINE_NAME = \"bpr_field_7\"\n",
    "\n",
    "# DATA_FILE_NAME = \"../Data/20230813T210908_sales_6mo_basic_single_events_removed.csv\"\n",
    "DATA_FILE_NAME = \"../Data/20230811T031507_sales_12mo_basic_single_events_removed.csv\"\n",
    "# DATA_FILE_NAME = \"../Data/20230809T003134_sales_25mo_basic_single_events_removed.csv\"\n",
    "COL_USER = \"location_id\"\n",
    "COL_ITEM = \"product\"\n",
    "COL_RATING = \"sold_revenue\"\n",
    "\n",
    "# country\n",
    "COUNTRY = \"nigeria\"\n",
    "\n",
    "# top k items to recommend, for train & test\n",
    "TOP_K_SPLIT_TRAIN_TEST = 10\n",
    "# top k items to recommend, for final product recommendation output\n",
    "TOP_K_WHOLE = 10\n",
    "SAVE_ALL_RECS = False\n",
    "SAVE_NEW_RECS = False\n",
    "\n",
    "# fraction of location_skus to include in training dataset\n",
    "TRAIN_FRAC = 0.75\n",
    "\n",
    "# top MULTIPLIER * k items are considered relevant for nDCG\n",
    "RNDCG_MULTIPLIER = 3\n",
    "\n",
    "# Model parameters\n",
    "NUM_FACTORS = 200\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.01\n",
    "LAMBDA_REG = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revised/copied code from packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rating_column(relevancy_method: str, **kwargs) -> str:\n",
    "    r\"\"\"Helper utility to simplify the arguments of eval metrics\n",
    "    Attemtps to address https://github.com/microsoft/recommenders/issues/1737.\n",
    "\n",
    "    Args:\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "\n",
    "    Returns:\n",
    "        str: rating column name.\n",
    "    \"\"\"\n",
    "    if relevancy_method != \"top_k\":\n",
    "        if \"col_rating\" not in kwargs:\n",
    "            raise ValueError(\"Expected an argument `col_rating` but wasn't found.\")\n",
    "        col_rating = kwargs.get(\"col_rating\")\n",
    "    else:\n",
    "        col_rating = kwargs.get(\"col_rating\", DEFAULT_RATING_COL)\n",
    "    return col_rating\n",
    "\n",
    "\n",
    "def get_top_k_items(\n",
    "    dataframe, col_user=DEFAULT_USER_COL, col_rating=DEFAULT_RATING_COL, k=DEFAULT_K\n",
    "):\n",
    "    \"\"\"Get the input customer-item-rating tuple in the format of Pandas\n",
    "    DataFrame, output a Pandas DataFrame in the dense format of top k items\n",
    "    for each user.\n",
    "\n",
    "    Note:\n",
    "        If it is implicit rating, just append a column of constants to be\n",
    "        ratings.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): DataFrame of rating data (in the format\n",
    "        customerID-itemID-rating)\n",
    "        col_user (str): column name for user\n",
    "        col_rating (str): column name for rating\n",
    "        k (int or None): number of items for each user; None means that the input has already been\n",
    "        filtered out top k items and sorted by ratings and there is no need to do that again.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame of top k items for each user, sorted by `col_user` and `rank`\n",
    "    \"\"\"\n",
    "    # Sort dataframe by col_user and (top k) col_rating\n",
    "    if k is None:\n",
    "        top_k_items = dataframe\n",
    "    else:\n",
    "        top_k_items = (\n",
    "            dataframe.sort_values([col_user, col_rating], ascending=[True, False])\n",
    "            .groupby(col_user, as_index=False)\n",
    "            .head(k)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    # Add ranks\n",
    "    top_k_items[\"rank\"] = top_k_items.groupby(col_user, sort=False).cumcount() + 1\n",
    "    return top_k_items\n",
    "\n",
    "\n",
    "def merge_ranking_true_pred_new(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user,\n",
    "    col_item,\n",
    "    col_rating,\n",
    "    col_prediction,\n",
    "    relevancy_method,\n",
    "    k=DEFAULT_K,\n",
    "    rndcg_multiplier=DEFAULT_RNDCG_MULTIPLIER,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "):\n",
    "    \"\"\"Filter truth and prediction data frames on common users\n",
    "\n",
    "    Args:\n",
    "        rating_true (pandas.DataFrame): True DataFrame\n",
    "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
    "        col_user (str): column name for user\n",
    "        col_item (str): column name for item\n",
    "        col_rating (str): column name for rating\n",
    "        col_prediction (str): column name for prediction\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "        k (int): number of top k items per user (optional)\n",
    "        threshold (float): threshold of top items per user (optional)\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame of recommendation hits, sorted by `col_user` and `rank`\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the prediction and true data frames have the same set of users\n",
    "    common_users = set(rating_true[col_user]).intersection(set(rating_pred[col_user]))\n",
    "    rating_true_common = rating_true[rating_true[col_user].isin(common_users)]\n",
    "    rating_pred_common = rating_pred[rating_pred[col_user].isin(common_users)]\n",
    "\n",
    "    # Return hit items in prediction data frame with ranking information. This is used for calculating NDCG and MAP.\n",
    "    # Use first to generate unique ranking values for each item. This is to align with the implementation in\n",
    "    # Spark evaluation metrics, where index of each recommended items (the indices are unique to items) is used\n",
    "    # to calculate penalized precision of the ordered items.\n",
    "    if relevancy_method == \"top_k\":\n",
    "        top_k = k\n",
    "    elif relevancy_method == \"by_threshold\":\n",
    "        top_k = threshold\n",
    "    elif relevancy_method is None:\n",
    "        top_k = None\n",
    "    else:\n",
    "        raise NotImplementedError(\"Invalid relevancy_method\")\n",
    "    df_hit = get_top_k_items(\n",
    "        dataframe=rating_pred_common,\n",
    "        col_user=col_user,\n",
    "        col_rating=col_prediction,\n",
    "        k=top_k,\n",
    "    )\n",
    "    rating_true_common_top_mult_k = get_top_k_items(\n",
    "        dataframe=rating_true_common,\n",
    "        col_user=col_user,\n",
    "        col_rating=col_rating,\n",
    "        k=RNDCG_MULTIPLIER * top_k,\n",
    "    )[[col_user, col_item]]\n",
    "    df_hit = pd.merge(df_hit, rating_true_common_top_mult_k, on=[col_user, col_item])[\n",
    "        [col_user, col_item, \"rank\"]\n",
    "    ]\n",
    "\n",
    "    return df_hit\n",
    "\n",
    "\n",
    "def rndcg_at_k(\n",
    "    rating_true,\n",
    "    rating_pred,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=DEFAULT_K,\n",
    "    rndcg_multiplier=RNDCG_MULTIPLIER,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "    score_type=\"binary\",\n",
    "    discfun_type=\"loge\",\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain (nDCG).\n",
    "\n",
    "    Info: https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
    "\n",
    "    Args:\n",
    "        rating_true (pandas.DataFrame): True DataFrame\n",
    "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
    "        col_user (str): column name for user\n",
    "        col_item (str): column name for item\n",
    "        col_rating (str): column name for rating\n",
    "        col_prediction (str): column name for prediction\n",
    "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
    "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
    "        k (int): number of top k items per user\n",
    "        threshold (float): threshold of top items per user (optional)\n",
    "        score_type (str): type of relevance scores ['binary', 'raw', 'exp']. With the default option 'binary', the\n",
    "            relevance score is reduced to either 1 (hit) or 0 (miss). Option 'raw' uses the raw relevance score.\n",
    "            Option 'exp' uses (2 ** RAW_RELEVANCE - 1) as the relevance score\n",
    "        discfun_type (str): type of discount function ['loge', 'log2'] used to calculate DCG.\n",
    "\n",
    "    Returns:\n",
    "        float: nDCG at k (min=0, max=1).\n",
    "    \"\"\"\n",
    "    col_rating = _get_rating_column(relevancy_method, **kwargs)\n",
    "    df_hit = merge_ranking_true_pred_new(\n",
    "        rating_true=rating_true,\n",
    "        rating_pred=rating_pred,\n",
    "        col_user=col_user,\n",
    "        col_item=col_item,\n",
    "        col_rating=col_rating,\n",
    "        col_prediction=col_prediction,\n",
    "        relevancy_method=relevancy_method,\n",
    "        k=k,\n",
    "        rndcg_multiplier=rndcg_multiplier,\n",
    "        threshold=threshold,\n",
    "    )\n",
    "\n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    df_dcg = df_hit.merge(rating_pred, on=[col_user, col_item]).merge(\n",
    "        rating_true, on=[col_user, col_item], how=\"outer\", suffixes=(\"_left\", None)\n",
    "    )\n",
    "\n",
    "    if score_type == \"binary\":\n",
    "        df_dcg[\"rel\"] = 1\n",
    "    elif score_type == \"raw\":\n",
    "        df_dcg[\"rel\"] = df_dcg[col_rating]\n",
    "    elif score_type == \"exp\":\n",
    "        df_dcg[\"rel\"] = 2 ** df_dcg[col_rating] - 1\n",
    "    else:\n",
    "        raise ValueError(\"score_type must be one of 'binary', 'raw', 'exp'\")\n",
    "\n",
    "    if discfun_type == \"loge\":\n",
    "        discfun = np.log\n",
    "    elif discfun_type == \"log2\":\n",
    "        discfun = np.log2\n",
    "    else:\n",
    "        raise ValueError(\"discfun_type must be one of 'loge', 'log2'\")\n",
    "\n",
    "    # Calculate the actual discounted gain for each record\n",
    "    df_dcg[\"dcg\"] = df_dcg[\"rel\"] / discfun(1 + df_dcg[\"rank\"])\n",
    "\n",
    "    # Calculate the ideal discounted gain for each record\n",
    "    df_idcg = df_dcg.sort_values([col_user, col_rating], ascending=False)\n",
    "    df_idcg[\"irank\"] = df_idcg.groupby(col_user, as_index=False, sort=False)[\n",
    "        col_rating\n",
    "    ].rank(\"first\", ascending=False)\n",
    "    df_idcg[\"idcg\"] = df_idcg[\"rel\"] / discfun(1 + df_idcg[\"irank\"])\n",
    "\n",
    "    # Calculate the actual DCG for each user\n",
    "    df_user = df_dcg.groupby(col_user, as_index=False, sort=False).agg({\"dcg\": \"sum\"})\n",
    "\n",
    "    # Calculate the ideal DCG for each user\n",
    "    df_user = df_user.merge(\n",
    "        df_idcg.groupby(col_user, as_index=False, sort=False)\n",
    "        .head(k)\n",
    "        .groupby(col_user, as_index=False, sort=False)\n",
    "        .agg({\"idcg\": \"sum\"}),\n",
    "        on=col_user,\n",
    "    )\n",
    "\n",
    "    # DCG over IDCG is the normalized DCG\n",
    "    df_user[\"ndcg\"] = df_user[\"dcg\"] / df_user[\"idcg\"]\n",
    "    return df_user[\"ndcg\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data\n",
    "\n",
    "\n",
    "### 1.1 Load and split data\n",
    "\n",
    "To evaluate the performance of item recommendation, we adopted the provided `python_random_split` tool for the consistency.  Data is randomly split into training and test sets with the ratio of 75/25.\n",
    "\n",
    "\n",
    "Note that Cornac also cover different [built-in schemes](https://cornac.readthedocs.io/en/latest/eval_methods.html) for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/g0trckmn61v44xkm8xvfx0n00000gn/T/ipykernel_11230/3664269612.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[COL_RATING] = data[\"sold_count\"] * data[\"price\"]\n",
      "/var/folders/pz/g0trckmn61v44xkm8xvfx0n00000gn/T/ipykernel_11230/3664269612.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(labels=[\"sold_count\", \"price\"], axis=1, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>product</th>\n",
       "      <th>sold_revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"f3c08845-37cd-420a-819c-653cfad4df7e\"</td>\n",
       "      <td>Coartem 80/480mg Tablets x6</td>\n",
       "      <td>144900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"d46d529e-e728-4d4e-b1a5-6512c894f43d\"</td>\n",
       "      <td>Coartem 80/480mg Tablets x6</td>\n",
       "      <td>16560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"67353226-2229-4b3b-b6aa-919fcc6bd7ff\"</td>\n",
       "      <td>Coartem 80/480mg Tablets x6</td>\n",
       "      <td>35190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"90dbea29-ee13-4aaf-8bda-e578befc20f7\"</td>\n",
       "      <td>Coartem 80/480mg Tablets x6</td>\n",
       "      <td>2070.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"f3686258-8c08-4918-9bbb-545618397f3e\"</td>\n",
       "      <td>Coartem 80/480mg Tablets x6</td>\n",
       "      <td>113850.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              location_id                      product  \\\n",
       "0  \"f3c08845-37cd-420a-819c-653cfad4df7e\"  Coartem 80/480mg Tablets x6   \n",
       "1  \"d46d529e-e728-4d4e-b1a5-6512c894f43d\"  Coartem 80/480mg Tablets x6   \n",
       "2  \"67353226-2229-4b3b-b6aa-919fcc6bd7ff\"  Coartem 80/480mg Tablets x6   \n",
       "3  \"90dbea29-ee13-4aaf-8bda-e578befc20f7\"  Coartem 80/480mg Tablets x6   \n",
       "4  \"f3686258-8c08-4918-9bbb-545618397f3e\"  Coartem 80/480mg Tablets x6   \n",
       "\n",
       "   sold_revenue  \n",
       "0      144900.0  \n",
       "1       16560.0  \n",
       "2       35190.0  \n",
       "3        2070.0  \n",
       "4      113850.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all_cols = pd.read_csv(DATA_FILE_NAME)\n",
    "data_all_cols = data_all_cols[data_all_cols[\"country\"] == COUNTRY]\n",
    "\n",
    "data = data_all_cols[[COL_USER, COL_ITEM, \"sold_count\", \"price\"]]\n",
    "# data[data[\"price\"] == 0].head()\n",
    "data[COL_RATING] = data[\"sold_count\"] * data[\"price\"]\n",
    "data.drop(labels=[\"sold_count\", \"price\"], axis=1, inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use quantity sold, not revenue of sales, for each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_random_split(data, TRAIN_FRAC, seed = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cornac Dataset\n",
    "\n",
    "To work with models implemented in Cornac, we need to construct an object from [Dataset](https://cornac.readthedocs.io/en/latest/data.html#module-cornac.data.dataset) class.\n",
    "\n",
    "Dataset Class in Cornac serves as the main object that the models will interact with.  In addition to data transformations, Dataset provides a bunch of useful iterators for looping through the data, as well as supporting different negative sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 588\n",
      "Number of items: 1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DanielShen/opt/anaconda3/envs/microsoft_rec/lib/python3.8/site-packages/cornac/data/dataset.py:361: UserWarning: 87 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    }
   ],
   "source": [
    "train_set = cornac.data.Dataset.from_uir(train.itertuples(index=False), seed=SEED)\n",
    "\n",
    "print('Number of users: {}'.format(train_set.num_users))\n",
    "print('Number of items: {}'.format(train_set.num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training\n",
    "\n",
    "The BPR has a few important parameters that we need to consider:\n",
    "\n",
    "- `k`: controls the dimension of the latent space (i.e. the size of the vectors  $w_u$  and  $h_i$ ).\n",
    "- `max_iter`: defines the number of iterations of the SGD procedure.\n",
    "- `learning_rate`: controls the step size $\\alpha$ in the gradient update rules.\n",
    "- `lambda_reg`: controls the L2-Regularization $\\lambda$ in the objective function.\n",
    "\n",
    "Note that different values of `k` and `max_iter` will affect the training time.\n",
    "\n",
    "We will here set `k` to 200, `max_iter` to 100, `learning_rate` to 0.01, and `lambda_reg` to 0.001. To train the model, we simply need to call the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr = cornac.models.BPR(\n",
    "    k=NUM_FACTORS,\n",
    "    max_iter=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lambda_reg=LAMBDA_REG,\n",
    "    verbose=True,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 62.58it/s, correct=94.49%, skipped=9.51%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished!\n",
      "Took 1.6515 seconds for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    bpr.fit(train_set)\n",
    "print(\"Took {} seconds for training.\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Prediction\n",
    "\n",
    "Now that our model is trained, we can produce the ranked lists for recommendation.  Every recommender models in Cornac provide `rate()` and `rank()` methods for predicting item rated value as well as item ranked list for a given user.  To make use of the current evaluation schemes, we will through `predict()` and `predict_ranking()` functions inside `cornac_utils` to produce the predictions.\n",
    "\n",
    "Note that BPR model is effectively designed for item ranking.  Hence, we only measure the performance using ranking metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.7000 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    all_predictions = predict_ranking(bpr, train, usercol=COL_USER, itemcol=COL_ITEM, remove_seen=True)\n",
    "print(\"Took {} seconds for prediction.\".format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>product</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71271</th>\n",
       "      <td>\"c56dab4f-69da-4bb4-9748-30c68216da6d\"</td>\n",
       "      <td>Flu-J Cough Syrup 100ml</td>\n",
       "      <td>-1.434742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71272</th>\n",
       "      <td>\"c56dab4f-69da-4bb4-9748-30c68216da6d\"</td>\n",
       "      <td>Secwid 500mg Tablets x3</td>\n",
       "      <td>2.806589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71273</th>\n",
       "      <td>\"c56dab4f-69da-4bb4-9748-30c68216da6d\"</td>\n",
       "      <td>Loxagyl 60ml Suspension</td>\n",
       "      <td>0.002896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71274</th>\n",
       "      <td>\"c56dab4f-69da-4bb4-9748-30c68216da6d\"</td>\n",
       "      <td>WAIPA 30/225mg Tablets x12</td>\n",
       "      <td>4.769132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71275</th>\n",
       "      <td>\"c56dab4f-69da-4bb4-9748-30c68216da6d\"</td>\n",
       "      <td>Parkalin Chesty Cough</td>\n",
       "      <td>-2.603724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  location_id                     product  \\\n",
       "71271  \"c56dab4f-69da-4bb4-9748-30c68216da6d\"     Flu-J Cough Syrup 100ml   \n",
       "71272  \"c56dab4f-69da-4bb4-9748-30c68216da6d\"     Secwid 500mg Tablets x3   \n",
       "71273  \"c56dab4f-69da-4bb4-9748-30c68216da6d\"     Loxagyl 60ml Suspension   \n",
       "71274  \"c56dab4f-69da-4bb4-9748-30c68216da6d\"  WAIPA 30/225mg Tablets x12   \n",
       "71275  \"c56dab4f-69da-4bb4-9748-30c68216da6d\"       Parkalin Chesty Cough   \n",
       "\n",
       "       prediction  \n",
       "71271   -1.434742  \n",
       "71272    2.806589  \n",
       "71273    0.002896  \n",
       "71274    4.769132  \n",
       "71275   -2.603724  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Evaluation / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.071958\n",
      "NDCG:\t0.435576\n",
      "RNDCG:\t0.316980\n",
      "Precision@K:\t0.421197\n",
      "Recall@K:\t0.107287\n"
     ]
    }
   ],
   "source": [
    "eval_map = map_at_k(test, all_predictions, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING, k=TOP_K_SPLIT_TRAIN_TEST)\n",
    "eval_ndcg = ndcg_at_k(test, all_predictions, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING, k=TOP_K_SPLIT_TRAIN_TEST)\n",
    "eval_rndcg = rndcg_at_k(test, all_predictions, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING, k=TOP_K_SPLIT_TRAIN_TEST, rndcg_multiplier=RNDCG_MULTIPLIER)\n",
    "eval_precision = precision_at_k(test, all_predictions, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING, k=TOP_K_SPLIT_TRAIN_TEST)\n",
    "eval_recall = recall_at_k(test, all_predictions, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING, k=TOP_K_SPLIT_TRAIN_TEST)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"RNDCG:\\t%f\" % eval_rndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Train, Predict, and Evaluate on Whole Dataset\n",
    "\n",
    "Earlier, we had split train (for model training) and test (for evaluation). In implementation, we have train = whole dataset, and we can evaluate on test = whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/DanielShen/opt/anaconda3/envs/microsoft_rec/lib/python3.8/site-packages/cornac/data/dataset.py:361: UserWarning: 153 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 588\n",
      "Number of items: 1948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 45.99it/s, correct=96.11%, skipped=12.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished!\n",
      "Took 2.1867 seconds for training.\n",
      "Took 0.2460 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "## Data\n",
    "data_set = cornac.data.Dataset.from_uir(data.itertuples(index=False), seed=SEED)\n",
    "print('Number of users: {}'.format(data_set.num_users))\n",
    "print('Number of items: {}'.format(data_set.num_items))\n",
    "\n",
    "## Train\n",
    "bpr_whole = cornac.models.BPR(\n",
    "    k=NUM_FACTORS,\n",
    "    max_iter=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lambda_reg=LAMBDA_REG,\n",
    "    verbose=True,\n",
    "    seed=SEED\n",
    ")\n",
    "with Timer() as t:\n",
    "    bpr_whole.fit(data_set)\n",
    "print(\"Took {} seconds for training.\".format(t))\n",
    "\n",
    "## Predict\n",
    "with Timer() as t:\n",
    "    all_predictions_whole = predict_ranking(bpr_whole, data, usercol=COL_USER, itemcol=COL_ITEM, remove_seen=False)\n",
    "print(\"Took {} seconds for prediction.\".format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.299101\n",
      "NDCG:\t0.676351\n",
      "RNDCG:\t0.667899\n",
      "Precision@K:\t0.603605\n",
      "Recall@K:\t0.436593\n"
     ]
    }
   ],
   "source": [
    "# TOP_K_WHOLE = 100\n",
    "# RNDCG_MULTIPLIER = 3\n",
    "\n",
    "## Evaluate\n",
    "eval_map_whole = map_at_k(data, all_predictions_whole, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING, k=TOP_K_WHOLE)\n",
    "eval_ndcg_whole = ndcg_at_k(data, all_predictions_whole, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING, k=TOP_K_WHOLE)\n",
    "eval_rndcg_whole = rndcg_at_k(data, all_predictions_whole, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING, k=TOP_K_WHOLE, rndcg_multiplier=RNDCG_MULTIPLIER)\n",
    "eval_precision_whole = precision_at_k(data, all_predictions_whole, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING, k=TOP_K_WHOLE)\n",
    "eval_recall_whole = recall_at_k(data, all_predictions_whole, col_user=COL_USER, col_item=COL_ITEM, col_rating=COL_RATING, k=TOP_K_WHOLE)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map_whole,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg_whole,\n",
    "      \"RNDCG:\\t%f\" % eval_rndcg_whole,\n",
    "      \"Precision@K:\\t%f\" % eval_precision_whole,\n",
    "      \"Recall@K:\\t%f\" % eval_recall_whole, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/g0trckmn61v44xkm8xvfx0n00000gn/T/ipykernel_11230/23401281.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_extra[COL_RATING] = data_extra[\"sold_count\"] * data_extra[\"price\"]\n"
     ]
    }
   ],
   "source": [
    "# SAVE_ALL_RECS = True\n",
    "# SAVE_NEW_RECS = True\n",
    "\n",
    "# Helper portion: get table of median price of every product\n",
    "if SAVE_ALL_RECS or SAVE_NEW_RECS:\n",
    "    prod_prices = data_all_cols[[COL_ITEM, \"price\"]]\n",
    "    prod_prices = prod_prices.groupby(COL_ITEM, sort=False).median()\n",
    "    prod_prices.rename(columns = {\"price\": \"median price\"}, inplace=True)\n",
    "    data_extra = data_all_cols[[COL_USER, COL_ITEM, \"sold_count\", \"price\"]]\n",
    "    data_extra[COL_RATING] = data_extra[\"sold_count\"] * data_extra[\"price\"]\n",
    "\n",
    "## Save recommendations\n",
    "if SAVE_ALL_RECS:\n",
    "    top_k_predictions_whole = get_top_k_items(all_predictions_whole, col_user=COL_USER, col_rating=DEFAULT_PREDICTION_COL, k=TOP_K_WHOLE)\n",
    "    top_k_predictions_whole.drop(DEFAULT_PREDICTION_COL, axis = 1, inplace = True)\n",
    "    # add column of true sales, price, and rank of predicted products\n",
    "    top_k_predictions_whole = top_k_predictions_whole.merge(prod_prices, how = 'left', on = [COL_ITEM])\n",
    "    top_all_true = get_top_k_items(data_extra, col_user=COL_USER, col_rating=COL_RATING, k=data_set.num_items)\n",
    "    top_k_predictions_whole = top_k_predictions_whole.merge(top_all_true.drop(\"price\", axis=1), how = 'left', on = [COL_USER, COL_ITEM], suffixes = (None, \"_true\"))\n",
    "    top_k_predictions_whole[\"rank_true\"] = top_k_predictions_whole[\"rank_true\"].convert_dtypes()\n",
    "    top_k_predictions_whole.rename(columns =\\\n",
    "        {COL_ITEM: \"predicted \" + COL_ITEM,\\\n",
    "        COL_RATING: \"predicted \" + COL_ITEM + \"'s true \" + COL_RATING,\\\n",
    "        \"rank_true\": \"predicted \" + COL_ITEM + \"'s true rank\",\\\n",
    "        \"sold_count\": \"predicted \" + COL_ITEM + \"'s true sold_count\",\\\n",
    "        \"median price\": \"predicted \" + COL_ITEM + \"'s median price\"\\\n",
    "        }, inplace = True)\n",
    "    # add columns of true top-ranked products\n",
    "    top_all_true.rename(columns =\\\n",
    "        {COL_ITEM: \"true \" + COL_ITEM,\\\n",
    "        COL_RATING: \"true \"  + COL_ITEM + \"'s \" + COL_RATING,\\\n",
    "        \"sold_count\": \"true \" + COL_ITEM + \"'s sold_count\",\\\n",
    "        \"price\": \"true \" + COL_ITEM + \"'s price\"}, inplace = True)\n",
    "    top_k_predictions_whole = top_k_predictions_whole.merge(top_all_true, how = 'left', on = [COL_USER, 'rank'])\n",
    "    # reorder columns\n",
    "    top_k_predictions_whole = top_k_predictions_whole\\\n",
    "        [[COL_USER, \"rank\", \"true \" + COL_ITEM, \"true \" + COL_ITEM + \"'s \" + COL_RATING,\\\n",
    "        \"true \" + COL_ITEM + \"'s sold_count\", \"true \" + COL_ITEM + \"'s price\",\\\n",
    "        \"predicted \" + COL_ITEM, \"predicted \"  + COL_ITEM + \"'s true \" + COL_RATING,\\\n",
    "        \"predicted \" + COL_ITEM + \"'s true rank\", \"predicted \" + COL_ITEM + \"'s true sold_count\",\\\n",
    "        \"predicted \" + COL_ITEM + \"'s median price\"]]\n",
    "    # save to csv\n",
    "    top_k_predictions_whole.to_csv(THIS_ENGINE_NAME + \"_\" + COUNTRY + \"_top_\" + str(TOP_K_WHOLE) + \"_all_prod_recs.csv\")\n",
    "\n",
    "if SAVE_NEW_RECS:\n",
    "    new_predictions_whole = all_predictions_whole.merge(data, on=[COL_USER,COL_ITEM], indicator=True, how=\"left\").query('_merge==\"left_only\"').drop('_merge', axis=1).drop([COL_RATING], axis=1)\n",
    "    top_k_predictions_whole = get_top_k_items(new_predictions_whole, col_user=COL_USER, col_rating=DEFAULT_PREDICTION_COL, k=TOP_K_WHOLE)\n",
    "    top_k_predictions_whole.drop(DEFAULT_PREDICTION_COL, axis = 1, inplace = True)\n",
    "    # add column of true price of predicted products\n",
    "    top_k_predictions_whole = top_k_predictions_whole.merge(prod_prices, how = 'left', on = [COL_ITEM])\n",
    "    top_k_predictions_whole.rename(columns = {COL_ITEM: \"predicted new \" + COL_ITEM, \"median price\": \"predicted new \" + COL_ITEM + \"'s median price\"}, inplace=True)\n",
    "    # add columns of true top-ranked products\n",
    "    top_k_true = get_top_k_items(data_extra, col_user=COL_USER, col_rating=COL_RATING, k=TOP_K_WHOLE)\n",
    "    top_k_true.rename(columns =\\\n",
    "        {COL_ITEM: \"true \" + COL_ITEM,\\\n",
    "        COL_RATING: \"true \"  + COL_ITEM + \"'s \" + COL_RATING,\\\n",
    "        \"sold_count\": \"true \" + COL_ITEM + \"'s sold_count\",\\\n",
    "        \"price\": \"true \" + COL_ITEM + \"'s price\"}, inplace = True)\n",
    "    top_k_predictions_whole = top_k_predictions_whole.merge(top_k_true, on = [COL_USER, 'rank'])\n",
    "    # reorder columns\n",
    "    top_k_predictions_whole = top_k_predictions_whole\\\n",
    "        [[COL_USER, \"rank\", \"true \" + COL_ITEM, \"true \" + COL_ITEM + \"'s \" + COL_RATING,\\\n",
    "        \"true \" + COL_ITEM + \"'s sold_count\", \"true \" + COL_ITEM + \"'s price\",\\\n",
    "        \"predicted new \" + COL_ITEM, \"predicted new \" + COL_ITEM + \"'s median price\"]]\n",
    "    # save to csv\n",
    "    top_k_predictions_whole.to_csv(THIS_ENGINE_NAME + \"_\" + COUNTRY + \"_top_\" + str(TOP_K_WHOLE) + \"_new_prod_recs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
